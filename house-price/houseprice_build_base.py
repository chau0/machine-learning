# -*- coding: utf-8 -*-
"""HousePrice_build_base.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ORbRL9B8gRpPRRuo-PwW1tgHYZfa1oLN
"""

#import library
from googleapiclient.discovery import build
import io, os
from googleapiclient.http import MediaIoBaseDownload
from google.colab import auth
#authenticate for access to google driver
auth.authenticate_user()

drive_service = build('drive', 'v3')
#find kaggle.json file on google driver
results = drive_service.files().list(
        q="name = 'kaggle.json'", fields="files(id)").execute()
#copy file to current working folder
kaggle_api_key = results.get('files', [])
filename = "/content/.kaggle/kaggle.json"
os.makedirs(os.path.dirname(filename), exist_ok=True)
request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])
fh = io.FileIO(filename, 'wb')
downloader = MediaIoBaseDownload(fh, request)
done = False
while done is False:
    status, done = downloader.next_chunk()
    print("Download %d%%." % int(status.progress() * 100))
os.chmod(filename, 600)

#install kaggle API
!pip install  kaggle

#copy kaggle.json to home for kaggle API detect
!mkdir  ~/.kaggle
!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json
#!cp /content/.kaggle/kaggle.json  /content/

#download house price competition data
!kaggle competitions download -c house-prices-advanced-regression-techniques -p house_price
#!kaggle competitions download -c house-prices-advanced-regression-techniques

import math
def rmse(x,y): return math.sqrt(((x-y)**2).mean())

def print_score(m):
    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_val), y_val),
                m.score(X_train, y_train), m.score(X_val, y_val)]
    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)
    print(res)

# Imports
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics
from sklearn.impute import *
# %matplotlib inline

# Import the train and test set
train_raw = pd.read_csv('./house_price/train.csv')
test_raw = pd.read_csv('./house_price/test.csv')
train_raw.head()

"""##Plot missing data graph"""

total = train_raw.isnull().sum().sort_values(ascending=False)
percent = (train_raw.isnull().sum()/train_raw.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
f, ax = plt.subplots(figsize=(15, 6))
plt.xticks(rotation='90')
sns.barplot(x=missing_data.index, y=missing_data['Percent'])
plt.xlabel('Features', fontsize=15)
plt.ylabel('Percent of missing values', fontsize=15)
plt.title('Percent missing data by feature', fontsize=15)
missing_data.head()

train_raw.isnull().sum()[train_raw.isnull().sum() > 0]

"""##Drop features have so much missing"""

train_core = train_raw.drop(['PoolQC','Fence','MiscFeature','Alley','FireplaceQu'], axis=1)
test_core = test_raw.drop(['PoolQC','Fence','MiscFeature','Alley','FireplaceQu'], axis=1)

"""##Log SalePrice"""

train_core.SalePrice = np.log1p(train_core.SalePrice)

"""##One Hot Encoding"""

price_col = train_core.iloc[:, -1]
train_core = train_core.iloc[:,:-1]
train_ohe = pd.get_dummies(train_core)
test_ohe = pd.get_dummies(test_core)
train_ohe, test_ohe = train_ohe.align(test_ohe,
                                       join='left', 
                                            axis=1)

train_ohe.head()

test_ohe.head()

train_ohe.isnull().sum()[train_ohe.isnull().sum() > 0]

"""##Handle Missing Value"""

final_imputer = SimpleImputer()
X = train_ohe.values
X = pd.DataFrame(final_imputer.fit_transform(X))
y = price_col.values

test_ohe = pd.DataFrame(final_imputer.transform(test_ohe))
X_test = test_ohe.values

def split_vals(a,n): return a[:n].copy(), a[n:].copy()

n_valid = 300  # same as Kaggle's test set size
n_trn = len(X)-n_valid

X_train, X_val = split_vals(X, n_trn)
y_train, y_val = split_vals(y, n_trn)

X_train.shape, y_train.shape, X_val.shape, y_val.shape

# Fit Random Forest on Training Set
from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(n_estimators=40,random_state=123)
regressor.fit(X_train, y_train)

print_score(regressor)

# Get predictions on processed test dataset.
predictions = regressor.predict(X_test)

sub = pd.DataFrame()
test_ID = test_raw['Id']
sub['Id'] = test_ID
sub['SalePrice'] = np.exp(predictions)
sub.to_csv('submission.csv',index=False)

!kaggle competitions submit house-prices-advanced-regression-techniques -f submission.csv -m "one hot encoding"

